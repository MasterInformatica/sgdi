Archivo para ir plasmando las ideas antes de realizar las diapositivas:
Luisma random forest
Jesús C4.5 y C5.0 (público y privado)

* C4.5 (También llamado J48)
- C4.5 es un algoritmo usado para generar un árbol de decision desarrollado
  por Ross Quinlan.
- C4.5 es una extensión del algoritmo ID3.
- Se diferencia en la seleccion del atributo: Dejar que a_best sea el
  atributo con la ganancia de información normalizada más alta.
** Mejoras respecto ID3
*** Permite atributos continuos y discretos
A fin de manejar atributos continuos, C4.5 crea un umbral y luego se divide
la lista en aquellos cuyo valor de atributo es superior al umbral y los que
son menores o iguales a él.
*** Permite valores de atributos faltantes
C4.5 permite valores de los atributos para ser marcado como "?" para
faltantes. Los valores faltantes de los atributos simplemente no se usa en
los cálculos de la ganancia y la entropía.
*** Permite atributos con costos diferentes
*** Podando árboles después de la creación
C4.5 se remonta a través del árbol una vez que ha sido creado e intenta
eliminar las ramas que no ayudan, reemplazándolos con los nodos de hoja.

* C5.0
Licencia Comercial. Existe una para un solo proceso con licencia GPL.
** Mejoras respecto C4.5
*** Rendimiento
*** Memoria
*** Tamaño del árbol
*** Poda de atributos irrelevantes
*** Ponderación de atributos más precisa




* Random Forest
** Main idea
   La idea principal de los random forest es que, en vez de tener un único
   árbol de clasificación, tener varios árboles, y cuando se quiera
   clasificar una instancia, tener en cuenta el resultado de todos los
   árboles sobre esa instancia.
   Faltaría por aclarar varios temas:
     (a) Cómo generar varios árboles de decisión con el mismo conjunto de
     entrenamiento.
     (b) Cómo generar un árbol en concreto.
     (c) Una vez que se tienen todos los árboles, como utilizarlos para
     clasificar una nueva instancia.

** Como generar varios árboles (Bagging, o bootstrap aggregating)
   - La técnica usada es la de Bagging, o Bootstrap aggregating.
   - Dado un conjunto de entrenamiento D de tamaño n, se generan B nuevos
     subconjuntos de entrenamiento de tamaño n', mediante un muestreo
     aleatorio con reemplazamiento.
   - Con reemplazamiento significa que las muestras ya escogidas pueden
     volver a ser escogidas como muestras posteriores, sin ser marcadas ni
     nada parecido.
   - El número de subconjuntos escogidos B ( o árboles resultantes) es un
     parámetro libre y depende del problema y de la persona que diseñe el
     algoritmo.
   - NOTA: aunque el número B es un parámetro libre, no he encontrado
     ninguna referencia al valor de n'.
   
** Como generar un árbol en concreto.
   - Se utiliza una técnica llamada "feature bagging".
   
   - La novedad de los random foresst es que para generar un árbol, en cada
     nodo split, en vez de decidir por qué atributo hacer la selección
     sobre el conjunto total de atributos restantes, se realiza sobre un
     conjunto de atributos aleatorios más pequeño.
   - El objetivo de esta técnica es que si un atributo es muy decisivo en
     un árbol, seguramente lo sea también en los otros árboles, produciendo
     que los árboles estén muy correlacionados, y caigan en los mismos
     errores.
   - El número de atributos aleatorios que se suele utilizar para decidir
     por qué atributo hacer el split, suele ser el siguiente:
       Si hay p atributos posibles, se escogen de manera aleatoria
     floor(sqrt(p)) atributos y se utiliza cualquier técnica de decisión
     (por ejemplo, aquel que aumenta más la ganancia de información como en ID3).

** Como decidir la clase de una instancia
   Una vez que se tienen todos los árboles y se desea clasificar una 
   instancia nueva, se calcula la clase que predicen todos los árboles para
   esa instancia, y se toma una decisión a partir de todos los resultados:
     - Si el problema es de clasificación (NUESTRO CASO), la clase
       final es aquella que más veces ha salido (la moda).
     - Si el problema es de regresión (una lástima que no lo hayamos visto
       en clase), se puede tomar la media aritmética de todas las medidas
       previstas, algo del estilo \frac{1}{N}*\sum_{i=1}^{N} f_x(i).

       
** Definiciones / ideas varias / palabros raros
   - Bagging (Bootstrap aggregating): Dado un conjunto de entrenamiento D
     de tamaño n, la técnica genera m nuevos conjuntos de entrenamiento de
     tamaño n'. Para generarlo, realiza un muestreo aleatorio por
     reemplazamiento.
   - Muestreo aleatorio con reemplazamiento: La idea es que las muestras
     escogidas para una selección pueden ser escogidas para selecciones
     posteriores, sin ser marcadas de ninguna manera especial.
     
** Ventajas e inconvenientes
   - Disminuye la varianza sin aumentar el rango de error. Esto significa
     que aunque las predicciones de un único árbol son altamente sensibles
     al ruido de su conjunto de entrenamiento, la media de todos los
     árboles no lo son. 
     
     
** TODO's
   - Mirar ventajas y desventajas.

** Extra
   - Existe un ExtraTree, que son árboles extra random, donde en cada nodo
     split, en vez de escoger el atributo que de más ganancia de
     información, se escoge de forma aleatoria. En algunos casos da
     resultados buenos.
     No se si esto mencionarlo o no. Se puede mencionar haciendo referencia
     al artículo. 

