Archivo para ir plasmando las ideas antes de realizar las diapositivas:
Luisma random forest
Jesús C4.5 y C5.0 (público y privado)


* Introduccion
Vamos a hablar de los algoritmos C4.5 y C5.0 que son mejoras al
algoritmo ID3 que vimos en clase y también hablaremos de "Random
Forest" que es un algoritmo de clasificación basado en bosques de
árboles de clasificación. Comenzamos:

** Recordar ID3
En primer lugar, vamos a recordar que es esto del ID3. Como vimos en
clase, dado un conjunto de instancias que tienen una clase podemos
construir un árbol que nos ayuda a clasificar nuevas instancias en
dichas clases. 

Para ello usábamos esta estructura de código que generaliza los TDIDT
(Top-Down I Decision Tree). Y el ID3 se caracterizaba por definir la
función "selecciona_atributo" como dar el atributo que nos da mayor
ganancia de información que es lo mismo que decir que el atributo deja
menor entropía después de dividir por él.

ID3 presenta una serie de limitaciones como puede ser el no admitir
atributos continuos...
* C4.5 (También llamado J48)
Viendo la estructura de código de los TDIDT y el algoritmo de ID3, se
plantea la necesidad de admitir valores de atributos continuos
(p.e. la temperatura de una ciudad). Esto nos lleva a los algoritmos
C4.X también desarrollados por JR Quinlan que mejorar sustancialmente
el ID3.

Este algoritmo también devuelve un árbol de clasificación e incluso
en ocasiones puede devolver el mismo. 
** Mejoras respecto ID3
La diferencia principal se podría decir que es la admisión de
atributos continuos en las instancias, pero no es la única. Vamos a
verlas:
*** Permite atributos continuos y discretos
A fin de manejar atributos continuos, C4.5 crea un umbral y luego se divide
la lista en aquellos cuyo valor de atributo es superior al umbral y los que
son menores o iguales a él.
*** Permite atributos con costos diferentes
Cada atributo tiene una peso.
*** Seleccion del atributo
Ahora la seleccion del atributo no se puede hacer tan directa dado
que los atributos continuos pueden dejar diferentes entropías
dependiendo de donde se haga el split del conjunto. Y además se tiene
en cuenta el peso asignado a cada atributo. En algunos papers denominan
a esta selección como elección del atributo que da mayor "ganancia de la
información NORMALIZADA".
*** Permite valores de atributos faltantes
C4.5 permite valores de los atributos para ser marcado como "?" para
faltantes. Los valores faltantes de los atributos simplemente no se usa en
los cálculos de la ganancia y la entropía.
*** Podando árboles después de la creación
Existen dos tipos de poda: previa y posterior. La previa la es como
la que realizamos en el ID3 al no desarrollar nodos si todas las
instancias tienen la misma clase, si no quedan instancias...
Y C4.5 añade una poda posterior de modo que se remonta a través del
árbol una vez que ha sido creado e intenta eliminar las ramas que no
ayudan, reemplazándolos con los nodos de hoja. Esto se utiliza para
eliminar ramas generadas por "ruido" de los datos.

Vamos a pasar a ejecutar en WEKA ambos algoritmos ID3 y C4.5
(conocido por J48). 

Como podemos observar para este conjunto de datos nos deja el mismo
árbol.. y para este otro no nos deja ejecutar el ID3 al tener valores
de atributos continuos.

* C5.0


Licencia Comercial. Existe una para un solo proceso con licencia GPL.
** Mejoras respecto C4.5
*** Rendimiento
*** Memoria
*** Tamaño del árbol
*** Poda de atributos irrelevantes
*** Ponderación de atributos más precisa


* Random Forest
** Main idea
   La idea principal de los random forest es que, en vez de tener un único
   árbol de clasificación, tener varios árboles, y cuando se quiera
   clasificar una instancia, tener en cuenta el resultado de todos los
   árboles sobre esa instancia.
   Faltaría por aclarar varios temas:
     (a) Cómo generar varios árboles de decisión con el mismo conjunto de
     entrenamiento.
     (b) Cómo generar un árbol en concreto.
     (c) Una vez que se tienen todos los árboles, como utilizarlos para
     clasificar una nueva instancia.

** Como generar varios árboles (Bagging, o bootstrap aggregating)
   - La técnica usada es la de Bagging, o Bootstrap aggregating.
   - Dado un conjunto de entrenamiento D de tamaño n, se generan B nuevos
     subconjuntos de entrenamiento de tamaño n', mediante un muestreo
     aleatorio con reemplazamiento.
   - Con reemplazamiento significa que las muestras ya escogidas pueden
     volver a ser escogidas como muestras posteriores, sin ser marcadas ni
     nada parecido.
   - El número de subconjuntos escogidos B ( o árboles resultantes) es un
     parámetro libre y depende del problema y de la persona que diseñe el
     algoritmo.
   - NOTA: aunque el número B es un parámetro libre, no he encontrado
     ninguna referencia al valor de n'.
   
** Como generar un árbol en concreto.
   - Se utiliza una técnica llamada "feature bagging".
   
   - La novedad de los random foresst es que para generar un árbol, en cada
     nodo split, en vez de decidir por qué atributo hacer la selección
     sobre el conjunto total de atributos restantes, se realiza sobre un
     conjunto de atributos aleatorios más pequeño.
   - El objetivo de esta técnica es que si un atributo es muy decisivo en
     un árbol, seguramente lo sea también en los otros árboles, produciendo
     que los árboles estén muy correlacionados, y caigan en los mismos
     errores.
   - El número de atributos aleatorios que se suele utilizar para decidir
     por qué atributo hacer el split, suele ser el siguiente:
       Si hay p atributos posibles, se escogen de manera aleatoria
     floor(sqrt(p)) atributos y se utiliza cualquier técnica de decisión
     (por ejemplo, aquel que aumenta más la ganancia de información como en ID3).

** Como decidir la clase de una instancia
   Una vez que se tienen todos los árboles y se desea clasificar una 
   instancia nueva, se calcula la clase que predicen todos los árboles para
   esa instancia, y se toma una decisión a partir de todos los resultados:
     - Si el problema es de clasificación (NUESTRO CASO), la clase
       final es aquella que más veces ha salido (la moda).
     - Si el problema es de regresión (una lástima que no lo hayamos visto
       en clase), se puede tomar la media aritmética de todas las medidas
       previstas, algo del estilo \frac{1}{N}*\sum_{i=1}^{N} f_x(i).

       
** Definiciones / ideas varias / palabros raros
   - Bagging (Bootstrap aggregating): Dado un conjunto de entrenamiento D
     de tamaño n, la técnica genera m nuevos conjuntos de entrenamiento de
     tamaño n'. Para generarlo, realiza un muestreo aleatorio por
     reemplazamiento.
   - Muestreo aleatorio con reemplazamiento: La idea es que las muestras
     escogidas para una selección pueden ser escogidas para selecciones
     posteriores, sin ser marcadas de ninguna manera especial.
     
** Ventajas e inconvenientes
   - Disminuye la varianza sin aumentar el rango de error. Esto significa
     que aunque las predicciones de un único árbol son altamente sensibles
     al ruido de su conjunto de entrenamiento, la media de todos los
     árboles no lo son. 
     
     
** TODO's
   - Mirar ventajas y desventajas.

** Extra
   - Existe un ExtraTree, que son árboles extra random, donde en cada nodo
     split, en vez de escoger el atributo que de más ganancia de
     información, se escoge de forma aleatoria. En algunos casos da
     resultados buenos.
     No se si esto mencionarlo o no. Se puede mencionar haciendo referencia
     al artículo. 

