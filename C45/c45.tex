%=================================================
\sectionDark{ID3}
%=================================================
\begin{frame}
  \frametitle{TDIDT\footnote{Diapositiva sacada del temario de SGDI}}
\includegraphics[width=\textwidth]{./images/esquemaTDIDT.png}
\end{frame}

\begin{frame}
  \frametitle{ID3\footnote{Diapositiva sacada del temario de SGDI}}
\includegraphics[width=\textwidth]{./images/esquemaID3.png}
\end{frame}

%=================================================
\sectionDark{C4.5}
%=================================================
\begin{frame}
  \frametitle{C4.5}
  El algoritmo ID3 es mejorado por el C4.5. Esta mejora, aparte de la
  optimización de partes de código, incluye\footnote{Artículo con las bases del algoritmo \cite{quinlan1986induction}}:
  \begin{itemize}
  \item Permite atributos continuos.
  \item Permite dar un peso diferente a cada atributo.
  \item Permite a una instancia no tener definido un valor en sus
    atributos.
  \item Mejora la selección del atributo clasificador.
  \item Realiza una poda del árbol después de la creación.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Algoritmo C4.5\footnote{Obtenido del libro \cite{quilan2014c4} y su review \cite{salzberg1994c4}}}
  \begin{tikzpicture}
    \tikzpicdimlarge
    \only<1->{\node[] (def) at (0.5,6) {
        \begin{minipage}{0.9\textwidth}
          \textcolor{purple}{\textbf{Split y Funciones Test:}}
          Para divir en subconjuntos las instancias test se divide el
          dominio donde haya mayor ganancia de información. Esta
          división se traduce en funciones test de tipo $x > 40$ ó $x
          < 4$, y para atributos discretos $ x = soleado$. 
        \end{minipage}
      };}

    \only<2->{\node[] (def) at (0.5,2) {
        \begin{minipage}{0.9\textwidth}
          \textcolor{purple}{\textbf{Selección del atributo:}}
          La selección del atributo por el que dividir consiste en
          escoger el atributo con mayor ganancia de información
          normalizado y ponderado. \\

          Para ello, se tiene en cuenta la proporción de instancias
          que queda en cada rama para el atributo candidato y el peso
          de importancia dado a dicho atributo.
          Siendo $D$ el conjunto de instancias, la fórmula para el
          atributo $i$ queda:
          $$ Ganancia_i = Peso_i * \sum_j \frac{|D_j|}{|D|} * Info_j $$
        \end{minipage}
      };}

  \end{tikzpicture}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ejemplo --  ID3 vs C4.5 (atributos continuos)}
\begin{tikzpicture}
\tikzpicdimlarge
\only<1->{\node[] () at (0,9)
  {\includegraphics[width=0.65\textwidth]{images/ID3iris.png}};}
\only<2->{\node[] () at (0,5)
  {\includegraphics[width=0.65\textwidth]{images/C45tree.png}};}

\end{tikzpicture}
\end{frame}

\begin{frame}
  \frametitle{C4.8 (J48)\footnote{Algoritmo utilizado en weka}: Mejoras a C4.5}
  \begin{tikzpicture}
    \tikzpicdimlarge
    \only<1->{\node[] (def) at (0.5,6) {
        \begin{minipage}{0.9\textwidth}
          \textcolor{purple}{\textbf{Problemas:}}\\
        ``The accuracy of T2's trees rivalled or surpassed C4.5's on 8
        of the 15 datasets, including all but one of the datasets
        having only continuous attributes.''\cite{auer1995theory}\\

        ``C4.5's performance was significantly improved on two data
        sets ... using the entropy discretization method and did no
        significantly degrade on any datasets [...] We conjeture that
        the C4.5 induction algorithm is not taking full advantage of
        possible local discretization.''\cite{dougherty1995supervised}
        \end{minipage}
      };}
    \only<2->{\node[] (def) at (0.5,2) {
        \begin{minipage}{0.9\textwidth}
          \textcolor{purple}{\textbf{Solución \cite{quinlan1996improved}:}}
          \begin{itemize}
            \item Aumentar el coste de usar atributos continuos
              disminuyendo el peso de importancia del atributo.
            \item Simplificar la división de un atributo continuo, en
              lugar de maximizar la ganacia, basta con superar un
              límite.
            \item Se añade el generador de reglas.
          \end{itemize}
        \end{minipage}
      };}
\end{tikzpicture}
\end{frame}

%=================================================
\sectionDark{C5.0}
%=================================================

\begin{frame}
  \frametitle{C5.0}
% Mejoras
  C5.0 tiene licencia comercial y licencia GPL para un solo proceso.\\
   \textcolor{purple}{\textbf{Mejoras \footnote{Basado en
         \cite{pandya2015c5} y la página oficial del autor
         \cite{quinlanURL}}:}}
\begin{itemize}
\item Aumenta el rendimiento del algoritmo.
\item Reduce el consumo de memoria local y total.
\item Devuelve árboles de clasificación reducidos.
\item Poda atributos irrelevantes para la clasificación.
\item Hace la ponderación de atributos más precisa.
\item Agrupa valores de atributos discretos en una misma rama.
\item Genera reglas con más acierto que las de C4.8.
\item Facilita la aplicación de técnicas como bagging y boosting,
  mejorando sus resultados.\footnote{\cite{freund1999alternating}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{C5.0: Example in R}

  \begin{tikzpicture}
    \tikzpicdimlarge
    \only<1>{\node[] (def) at (0.5,6) {
\begin{lstlisting}[frame=single, numbers=left]
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data"
crx <- read.table( file=url, header=FALSE, sep="," )
head( crx, 6 )
crx <- crx[ sample( nrow( crx ) ), ]
X <- crx[,1:15]
y <- crx[,16]
trainX <- X[1:600,]
trainy <- y[1:600]
testX <- X[601:690,]
testy <- y[601:690]
library(C50)
model <- C50::C5.0( trainX, trainy )
summary( model )
\end{lstlisting}
};}
\end{tikzpicture}
\end{frame}




